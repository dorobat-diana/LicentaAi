{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOXuo3NGiR50wCTlzuP/Mkc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dorobat-diana/LicentaAi/blob/main/SiameseFinetunning_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zwjq2YtOT54J",
        "outputId": "5d42db4f-3956-4675-9b40-cb6d845b6697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading the previously trained Siamese model...\n",
            "Trained Siamese model loaded successfully.\n",
            "Original Siamese model summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"siamese_network_v2_corrected_lambda\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"siamese_network_v2_corrected_lambda\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_image_A       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_image_B       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ feature_extractor_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │  \u001b[38;5;34m2,257,984\u001b[0m │ input_image_A[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_image_B[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ L1_distance         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ feature_extracto… │\n",
              "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │ feature_extracto… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dense_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m256\u001b[0m │ L1_distance[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dropout_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ head_dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dense_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ head_dropout_1[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dropout_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ head_dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ similarity_predict… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ head_dropout_2[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_image_A       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_image_B       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ feature_extractor_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │ input_image_A[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_image_B[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ L1_distance         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ feature_extracto… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │ feature_extracto… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dense_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ L1_distance[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dropout_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ head_dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dense_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ head_dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dropout_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ head_dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ similarity_predict… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ head_dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,266,561\u001b[0m (8.65 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,266,561</span> (8.65 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,577\u001b[0m (33.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,577</span> (33.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attempting to unfreeze TOP layers of the feature extractor...\n",
            "Found feature extractor layer: feature_extractor_functional of type <class 'keras.src.models.functional.Functional'>\n",
            "Set 'feature_extractor_functional' to non-trainable initially.\n",
            "'feature_extractor_functional' is a Keras Model. Unfreezing its top 5 layers.\n",
            "Successfully unfroze 3 non-BatchNormalization layers from the top 5 of 'feature_extractor_functional'.\n",
            "\n",
            "Siamese model summary AFTER unfreezing feature extractor part:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"siamese_network_v2_corrected_lambda\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"siamese_network_v2_corrected_lambda\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_image_A       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_image_B       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ feature_extractor_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │  \u001b[38;5;34m2,257,984\u001b[0m │ input_image_A[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_image_B[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ L1_distance         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ feature_extracto… │\n",
              "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │ feature_extracto… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dense_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m256\u001b[0m │ L1_distance[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dropout_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ head_dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dense_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ head_dropout_1[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dropout_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ head_dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ similarity_predict… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ head_dropout_2[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_image_A       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_image_B       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ feature_extractor_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │ input_image_A[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_image_B[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ L1_distance         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ feature_extracto… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │ feature_extracto… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dense_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ L1_distance[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dropout_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ head_dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dense_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ head_dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head_dropout_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ head_dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ similarity_predict… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ head_dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,266,561\u001b[0m (8.65 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,266,561</span> (8.65 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m418,177\u001b[0m (1.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">418,177</span> (1.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,848,384\u001b[0m (7.05 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,848,384</span> (7.05 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing training data (with augmentation)...\n",
            "Number of training pairs for fine-tuning: 18300\n",
            "\n",
            "Preparing validation data (no augmentation)...\n",
            "Number of validation pairs for fine-tuning: 18300\n",
            "\n",
            "Re-compiling Siamese model for fine-tuning...\n",
            "\n",
            "Fine-tuning Siamese model...\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mname 'K' is not defined\u001b[0m\n\nArguments received by Lambda.call():\n  • inputs=['tf.Tensor(shape=(None, 1280), dtype=float32)', 'tf.Tensor(shape=(None, 1280), dtype=float32)']\n  • mask=['None', 'None']\n  • training=True",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d4dcde175cdf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     history_finetune = siamese_model.fit(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtrain_dataset_finetune\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS_FINETUNE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/python_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(tensors)\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mname 'K' is not defined\u001b[0m\n\nArguments received by Lambda.call():\n  • inputs=['tf.Tensor(shape=(None, 1280), dtype=float32)', 'tf.Tensor(shape=(None, 1280), dtype=float32)']\n  • mask=['None', 'None']\n  • training=True"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 0. Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Google Drive already mounted or mount failed.\")\n",
        "\n",
        "# --- 1. Configuration & Paths ---\n",
        "# Path to the Siamese model trained in the FIRST phase\n",
        "TRAINED_SIAMESE_MODEL_PATH = '/content/drive/MyDrive/ColabNotebooks/results/siamese_mobilenetv2_landmark_model_v2.keras'\n",
        "# Path to save the model after this SECOND fine-tuning phase\n",
        "FINETUNED_SIAMESE_MODEL_PATH = '/content/drive/MyDrive/ColabNotebooks/results/siamese_mobilenetv2_landmark_model_v2_top_layers_finetuned.keras' # Changed name\n",
        "\n",
        "TRAIN_DIR = '/content/drive/MyDrive/ColabNotebooks/data/famous_places/split/train'\n",
        "TEST_DIR = '/content/drive/MyDrive/ColabNotebooks/data/famous_places/split/test' # For validation pairs\n",
        "\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
        "BATCH_SIZE = 16 # Consider reducing batch size if memory becomes an issue with more trainable params\n",
        "EPOCHS_FINETUNE = 25 # Adjust as needed, use EarlyStopping\n",
        "LEARNING_RATE_FINETUNE = 1e-5 # CRITICAL: Use a very low learning rate\n",
        "N_TOP_LAYERS_TO_UNFREEZE = 5 # Number of layers from the END of MobileNetV2 to unfreeze. Adjust this!\n",
        "                               # For MobileNetV2, a block can have multiple layers (Conv, BN, ReLU, DepthwiseConv etc.)\n",
        "                               # For example, unfreezing from 'block_13_expand' onwards.\n",
        "                               # Inspect your feature_extractor_layer_in_siamese.summary() to see layer names and count.\n",
        "\n",
        "# --- 2. Load Pre-Trained Siamese Model ---\n",
        "print(\"Loading the previously trained Siamese model...\")\n",
        "try:\n",
        "    tf.keras.config.enable_unsafe_deserialization()\n",
        "    siamese_model = load_model(TRAINED_SIAMESE_MODEL_PATH, compile=False)\n",
        "    print(\"Trained Siamese model loaded successfully.\")\n",
        "    # Assuming LEARNING_RATE_FINETUNE is defined as in your script\n",
        "    siamese_model.compile(\n",
        "      loss='binary_crossentropy',\n",
        "      optimizer=Adam(learning_rate=LEARNING_RATE_FINETUNE),\n",
        "      metrics=['accuracy']\n",
        "    )\n",
        "    print(\"Original Siamese model summary:\")\n",
        "    siamese_model.summary()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading the Siamese model: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 3. Unfreeze TOP Layers in the Feature Extractor part of the Siamese Model ---\n",
        "print(\"\\nAttempting to unfreeze TOP layers of the feature extractor...\")\n",
        "try:\n",
        "    feature_extractor_layer_in_siamese = siamese_model.get_layer(\"feature_extractor_functional\")\n",
        "    print(f\"Found feature extractor layer: {feature_extractor_layer_in_siamese.name} of type {type(feature_extractor_layer_in_siamese)}\")\n",
        "\n",
        "    # First, set the entire sub-model (feature_extractor) to non-trainable,\n",
        "    # then selectively unfreeze its top layers. This ensures only the intended layers are unfrozen.\n",
        "    feature_extractor_layer_in_siamese.trainable = False\n",
        "    print(f\"Set '{feature_extractor_layer_in_siamese.name}' to non-trainable initially.\")\n",
        "\n",
        "    # Check if this layer is a Keras Model itself, which should have a 'layers' attribute\n",
        "    if hasattr(feature_extractor_layer_in_siamese, 'layers') and isinstance(feature_extractor_layer_in_siamese, Model):\n",
        "        print(f\"'{feature_extractor_layer_in_siamese.name}' is a Keras Model. Unfreezing its top {N_TOP_LAYERS_TO_UNFREEZE} layers.\")\n",
        "\n",
        "        # It's good practice to keep BatchNormalization layers frozen when fine-tuning,\n",
        "        # especially if the new dataset/batch size is small, to use their pre-trained statistics.\n",
        "        # However, some argue that for end-to-end fine-tuning, they should also adapt.\n",
        "        # Let's try keeping them frozen first.\n",
        "        unfrozen_count = 0\n",
        "        for layer in feature_extractor_layer_in_siamese.layers[-N_TOP_LAYERS_TO_UNFREEZE:]:\n",
        "            # Crucial: Do NOT make Batch Normalization layers trainable when fine-tuning with small batch sizes\n",
        "            # or if you want to preserve the statistics learned from the larger pre-training dataset.\n",
        "            # MobileNetV2 heavily relies on Batch Norm.\n",
        "            if not isinstance(layer, BatchNormalization):\n",
        "                layer.trainable = True\n",
        "                unfrozen_count += 1\n",
        "                # print(f\"  Layer '{layer.name}' is now TRAINABLE.\")\n",
        "            # else:\n",
        "                # print(f\"  Layer '{layer.name}' (BatchNormalization) remains FROZEN.\")\n",
        "        print(f\"Successfully unfroze {unfrozen_count} non-BatchNormalization layers from the top {N_TOP_LAYERS_TO_UNFREEZE} of '{feature_extractor_layer_in_siamese.name}'.\")\n",
        "        if unfrozen_count == 0 and N_TOP_LAYERS_TO_UNFREEZE > 0:\n",
        "            print(f\"WARNING: No layers were unfrozen. Check N_TOP_LAYERS_TO_UNFREEZE ({N_TOP_LAYERS_TO_UNFREEZE}) or if all top layers are BatchNormalization.\")\n",
        "\n",
        "        # If you want to unfreeze Batch Norm layers as well (use with caution):\n",
        "        # for layer in feature_extractor_layer_in_siamese.layers[-N_TOP_LAYERS_TO_UNFREEZE:]:\n",
        "        #     layer.trainable = True\n",
        "        # print(f\"Unfroze all top {N_TOP_LAYERS_TO_UNFREEZE} layers of '{feature_extractor_layer_in_siamese.name}' (including BN).\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(f\"Warning: Layer '{feature_extractor_layer_in_siamese.name}' is not a Keras Model or has no 'layers' attribute. Cannot partially unfreeze its internal layers this way.\")\n",
        "        print(\"If this is unexpected, ensure 'feature_extractor_functional' was indeed a tf.keras.Model.\")\n",
        "        print(\"Setting the entire feature_extractor_layer_in_siamese to trainable as a fallback (original Option 1 behavior).\")\n",
        "        feature_extractor_layer_in_siamese.trainable = True\n",
        "\n",
        "\n",
        "    print(\"\\nSiamese model summary AFTER unfreezing feature extractor part:\")\n",
        "    siamese_model.summary()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during unfreezing process: {e}\")\n",
        "    print(\"Ensure the feature extractor model name 'feature_extractor_functional' matches how it was named and used in the first phase,\")\n",
        "    print(\"and that it is a tf.keras.Model instance.\")\n",
        "    raise\n",
        "\n",
        "\n",
        "# --- 4. Prepare Data (with Augmentation) ---\n",
        "\n",
        "def augment_image(image):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.15)\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image_tf_augmented(path_tensor, img_shape=(IMG_WIDTH, IMG_HEIGHT, 3), augment=False):\n",
        "    img = tf.io.read_file(path_tensor)\n",
        "    try:\n",
        "        img = tf.image.decode_image(img, channels=img_shape[2], expand_animations=False)\n",
        "    except tf.errors.InvalidArgumentError as e:\n",
        "        tf.print(f\"Warning: Could not decode image {path_tensor}. Error: {e}. Returning zeros.\")\n",
        "        return tf.zeros(img_shape, dtype=tf.float32)\n",
        "\n",
        "    if len(img.shape) != 3 or img.shape[2] != img_shape[2]:\n",
        "        tf.print(f\"Warning: Image {path_tensor} has unexpected shape {img.shape}. Converting to RGB or returning zeros.\")\n",
        "        if img.shape[2] == 1: img = tf.image.grayscale_to_rgb(img)\n",
        "        elif img.shape[2] == 4: img = img[:,:,:3]\n",
        "        else: return tf.zeros(img_shape, dtype=tf.float32)\n",
        "\n",
        "    img = tf.image.resize(img, [img_shape[0], img_shape[1]])\n",
        "    img = tf.cast(img, tf.float32)\n",
        "\n",
        "    if augment:\n",
        "        img = augment_image(img)\n",
        "\n",
        "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def list_image_files_and_labels(directory):\n",
        "    image_paths_by_label = {}\n",
        "    label_to_name = {}\n",
        "    name_to_label = {}\n",
        "    current_label_id = 0\n",
        "    class_names = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(directory, class_name)\n",
        "        if class_name not in name_to_label:\n",
        "            name_to_label[class_name] = current_label_id\n",
        "            label_to_name[current_label_id] = class_name\n",
        "            image_paths_by_label[current_label_id] = []\n",
        "            current_label_id += 1\n",
        "        label_id = name_to_label[class_name]\n",
        "        for fname in os.listdir(class_dir):\n",
        "            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                image_paths_by_label[label_id].append(os.path.join(class_dir, fname))\n",
        "    all_unique_labels = sorted(list(label_to_name.keys()))\n",
        "    return image_paths_by_label, label_to_name, name_to_label, all_unique_labels\n",
        "\n",
        "def create_image_pairs(image_paths_by_label, all_unique_labels, num_pairs_per_anchor=1):\n",
        "    pair_paths = []\n",
        "    pair_labels = []\n",
        "    if not all_unique_labels or not image_paths_by_label: return np.array(pair_paths), np.array(pair_labels)\n",
        "\n",
        "    all_images_flat = [{'path': p, 'label': lbl} for lbl, paths in image_paths_by_label.items() for p in paths]\n",
        "    if not all_images_flat: return np.array(pair_paths), np.array(pair_labels)\n",
        "\n",
        "    random.shuffle(all_images_flat)\n",
        "\n",
        "    for anchor_data in all_images_flat:\n",
        "        anchor_path, anchor_label = anchor_data['path'], anchor_data['label']\n",
        "\n",
        "        positive_candidates = [p for p in image_paths_by_label.get(anchor_label, []) if p != anchor_path]\n",
        "        if positive_candidates:\n",
        "            selected_positives = random.sample(positive_candidates, min(len(positive_candidates), num_pairs_per_anchor))\n",
        "            for positive_path in selected_positives:\n",
        "                pair_paths.append([anchor_path, positive_path])\n",
        "                pair_labels.append(1.0)\n",
        "\n",
        "        negative_label_choices = [l for l in all_unique_labels if l != anchor_label]\n",
        "        if negative_label_choices:\n",
        "            for _ in range(num_pairs_per_anchor):\n",
        "                negative_label = random.choice(negative_label_choices)\n",
        "                negative_candidates = image_paths_by_label.get(negative_label, [])\n",
        "                if negative_candidates:\n",
        "                    negative_path = random.choice(negative_candidates)\n",
        "                    pair_paths.append([anchor_path, negative_path])\n",
        "                    pair_labels.append(0.0)\n",
        "\n",
        "    combined = list(zip(pair_paths, pair_labels))\n",
        "    random.shuffle(combined)\n",
        "    if combined:\n",
        "        pair_paths, pair_labels = zip(*combined)\n",
        "    else:\n",
        "        return np.array([]), np.array([])\n",
        "    return np.array(pair_paths), np.array(pair_labels)\n",
        "\n",
        "\n",
        "def create_tf_dataset(pair_paths, pair_labels, batch_size, img_shape, augment_data=False):\n",
        "    if len(pair_paths) == 0:\n",
        "        print(\"Warning: pair_paths is empty. Cannot create tf.data.Dataset.\")\n",
        "        return None\n",
        "\n",
        "    path1_list = [p[0] for p in pair_paths]\n",
        "    path2_list = [p[1] for p in pair_paths]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(((path1_list, path2_list), pair_labels))\n",
        "\n",
        "    def _preprocess_pair(paths, label):\n",
        "        path1, path2 = paths\n",
        "        img1 = load_and_preprocess_image_tf_augmented(path1, img_shape, augment=augment_data)\n",
        "        img2 = load_and_preprocess_image_tf_augmented(path2, img_shape, augment=augment_data)\n",
        "        return (img1, img2), label\n",
        "\n",
        "    dataset = dataset.shuffle(buffer_size=len(pair_labels))\n",
        "    dataset = dataset.map(_preprocess_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "print(\"\\nPreparing training data (with augmentation)...\")\n",
        "train_images_by_label, _, _, train_unique_labels = list_image_files_and_labels(TRAIN_DIR)\n",
        "train_pair_paths, train_pair_labels = create_image_pairs(train_images_by_label, train_unique_labels, num_pairs_per_anchor=3)\n",
        "print(f\"Number of training pairs for fine-tuning: {len(train_pair_paths)}\")\n",
        "train_dataset_finetune = create_tf_dataset(train_pair_paths, train_pair_labels, BATCH_SIZE, IMG_SHAPE, augment_data=True)\n",
        "\n",
        "print(\"\\nPreparing validation data (no augmentation)...\")\n",
        "test_images_by_label, _, _, test_unique_labels = list_image_files_and_labels(TEST_DIR)\n",
        "val_pair_paths, val_pair_labels = create_image_pairs(test_images_by_label, test_unique_labels, num_pairs_per_anchor=3)\n",
        "print(f\"Number of validation pairs for fine-tuning: {len(val_pair_paths)}\")\n",
        "val_dataset_finetune = create_tf_dataset(val_pair_paths, val_pair_labels, BATCH_SIZE, IMG_SHAPE, augment_data=False)\n",
        "\n",
        "\n",
        "# --- 5. Re-Compile and Train the Siamese Model (Fine-tuning) ---\n",
        "if train_dataset_finetune and val_dataset_finetune:\n",
        "    print(\"\\nRe-compiling Siamese model for fine-tuning...\")\n",
        "    # CRITICAL: Re-compile the model to ensure changes in layer trainability take effect.\n",
        "    siamese_model.compile(loss='binary_crossentropy',\n",
        "                          optimizer=Adam(learning_rate=LEARNING_RATE_FINETUNE),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "    print(\"\\nFine-tuning Siamese model...\")\n",
        "    early_stopping_finetune = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=7,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    model_checkpoint_finetune = tf.keras.callbacks.ModelCheckpoint(\n",
        "        FINETUNED_SIAMESE_MODEL_PATH,\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    reduce_lr_finetune = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=3,\n",
        "        min_lr=1e-7, # Adjusted to allow even lower LR if needed\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    history_finetune = siamese_model.fit(\n",
        "        train_dataset_finetune,\n",
        "        epochs=EPOCHS_FINETUNE,\n",
        "        validation_data=val_dataset_finetune,\n",
        "        callbacks=[early_stopping_finetune, model_checkpoint_finetune, reduce_lr_finetune]\n",
        "    )\n",
        "\n",
        "    if history_finetune and history_finetune.history:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history_finetune.history['loss'], label='Train Loss (Fine-tune)')\n",
        "        plt.plot(history_finetune.history['val_loss'], label='Validation Loss (Fine-tune)')\n",
        "        plt.title('Fine-tuning Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history_finetune.history['accuracy'], label='Train Accuracy (Fine-tune)')\n",
        "        plt.plot(history_finetune.history['val_accuracy'], label='Validation Accuracy (Fine-tune)')\n",
        "        plt.title('Fine-tuning Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"/content/drive/MyDrive/ColabNotebooks/results/finetuning_v2_top_layers_history.png\") # Changed name\n",
        "        plt.show()\n",
        "\n",
        "    print(f\"Fine-tuned Siamese model (top layers) potentially saved to {FINETUNED_SIAMESE_MODEL_PATH} by ModelCheckpoint.\")\n",
        "    # Load the best model explicitly if ModelCheckpoint was used\n",
        "    print(\"Loading the best model saved by ModelCheckpoint...\")\n",
        "    siamese_model = load_model(FINETUNED_SIAMESE_MODEL_PATH)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Fine-tuning training and/or validation dataset could not be created. Skipping fine-tuning.\")\n",
        "\n",
        "\n",
        "# --- 6. How to Use for New Landmark Identification (Conceptual) ---\n",
        "def predict_similarity(img_path1, img_path2, model, img_shape):\n",
        "    if model is None:\n",
        "        print(\"Model not available for prediction.\")\n",
        "        return None\n",
        "    img1_tensor = load_and_preprocess_image_tf_augmented(tf.constant(img_path1), img_shape, augment=False)\n",
        "    img2_tensor = load_and_preprocess_image_tf_augmented(tf.constant(img_path2), img_shape, augment=False)\n",
        "\n",
        "    if tf.reduce_sum(img1_tensor) == 0 or tf.reduce_sum(img2_tensor) == 0:\n",
        "        print(f\"Warning: One or both images ({os.path.basename(img_path1)}, {os.path.basename(img_path2)}) might not have loaded correctly for prediction.\")\n",
        "\n",
        "    img1_batch = tf.expand_dims(img1_tensor, axis=0)\n",
        "    img2_batch = tf.expand_dims(img2_tensor, axis=0)\n",
        "\n",
        "    try:\n",
        "        prediction = model.predict([img1_batch, img2_batch], verbose=0)\n",
        "        return prediction[0][0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"\\nLoading the BEST fine-tuned model for final predictions...\")\n",
        "try:\n",
        "    # Ensure siamese_model holds the best version, especially if EarlyStopping restored weights\n",
        "    # and ModelCheckpoint also saved. Loading explicitly from ModelCheckpoint path is safest.\n",
        "    final_model_to_test = load_model(FINETUNED_SIAMESE_MODEL_PATH)\n",
        "    print(\"Best fine-tuned model loaded for testing.\")\n",
        "\n",
        "    if val_pair_paths is not None and len(val_pair_paths) > 0 and final_model_to_test is not None:\n",
        "        print(\"\\nExample predictions on validation pairs (using the BEST fine-tuned model):\")\n",
        "\n",
        "        positive_indices = [i for i, label in enumerate(val_pair_labels) if label == 1.0]\n",
        "        if positive_indices:\n",
        "            idx = random.choice(positive_indices)\n",
        "            path1, path2 = val_pair_paths[idx]\n",
        "            label = val_pair_labels[idx]\n",
        "            sim = predict_similarity(path1, path2, final_model_to_test, IMG_SHAPE)\n",
        "            if sim is not None:\n",
        "                print(f\"Pair: ({os.path.basename(path1)}, {os.path.basename(path2)}), Actual Label: {label}, Predicted Similarity: {sim:.4f} (Same class)\")\n",
        "        else:\n",
        "            print(\"No positive validation pairs found for example prediction.\")\n",
        "\n",
        "        negative_indices = [i for i, label in enumerate(val_pair_labels) if label == 0.0]\n",
        "        if negative_indices:\n",
        "            idx = random.choice(negative_indices)\n",
        "            path1, path2 = val_pair_paths[idx]\n",
        "            label = val_pair_labels[idx]\n",
        "            sim = predict_similarity(path1, path2, final_model_to_test, IMG_SHAPE)\n",
        "            if sim is not None:\n",
        "                print(f\"Pair: ({os.path.basename(path1)}, {os.path.basename(path2)}), Actual Label: {label}, Predicted Similarity: {sim:.4f} (Different classes)\")\n",
        "        else:\n",
        "            print(\"No negative validation pairs found for example prediction.\")\n",
        "    else:\n",
        "        print(\"\\nSkipping example predictions as validation pairs or fine-tuned model is not available.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or using the fine-tuned model for final predictions: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Fine-tuning Script (Top Layers) Finished ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (plotting code from Phase 2 history) ...\n",
        "SIAMESE_MODEL_PHASE2_PATH = '/content/drive/MyDrive/ColabNotebooks/results/siamese_mobilenetv2_landmark_model_phase2_toplayers.keras'\n",
        "print(f\"Phase 2 fine-tuned Siamese model saved to {SIAMESE_MODEL_PHASE2_PATH} (by ModelCheckpoint).\")\n",
        "print(\"This model was built with the corrected Lambda layer definition (with output_shape).\")\n",
        "\n",
        "# Load the best model saved by ModelCheckpoint for any immediate post-training use\n",
        "print(\"Loading the best model from Phase 2 fine-tuning...\")\n",
        "\n",
        "# --- MODIFICATION ---\n",
        "# Ensure unsafe deserialization is enabled before loading, as it contains a Python lambda.\n",
        "# This should ideally be set once at the beginning of the script if loading multiple such models,\n",
        "# but re-asserting it here or ensuring it's active is fine.\n",
        "siamese_model_phase2_loaded = None # Initialize to ensure it exists\n",
        "try:\n",
        "    tf.keras.config.enable_unsafe_deserialization()\n",
        "    print(\"Unsafe deserialization enabled (to load Python lambda in Lambda layer).\")\n",
        "\n",
        "    siamese_model_phase2_loaded = load_model(SIAMESE_MODEL_PHASE2_PATH) # Load into a new variable or overwrite 'siamese_model'\n",
        "    print(\"Best Phase 2 model loaded successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: Could not enable unsafe deserialization or load Phase 2 model: {e}\")\n",
        "    # siamese_model_phase2_loaded will remain None\n",
        "# --- END MODIFICATION ---\n",
        "\n",
        "# You can run predictions with this 'siamese_model_phase2_loaded' if needed.\n",
        "# It's good practice to check if the model was loaded successfully.\n",
        "if siamese_model_phase2_loaded is not None:\n",
        "    if val_pair_paths is not None and len(val_pair_paths) > 0: # Assuming val_pair_paths is still in scope\n",
        "        print(\"\\nExample predictions on validation pairs (using the Phase 2 fine-tuned model):\")\n",
        "        # ... (rest of your prediction code using siamese_model_phase2_loaded) ...\n",
        "        # For example, if your predict_similarity function and val_pair_paths are available:\n",
        "        # positive_indices = [i for i, label in enumerate(val_pair_labels) if label == 1.0]\n",
        "        # if positive_indices:\n",
        "        #     idx = random.choice(positive_indices)\n",
        "        #     path1, path2 = val_pair_paths[idx]\n",
        "        #     label_val = val_pair_labels[idx]\n",
        "        #     sim = predict_similarity(path1, path2, siamese_model_phase2_loaded, IMG_SHAPE)\n",
        "        #     if sim is not None:\n",
        "        #         print(f\"Pair: ({os.path.basename(path1)}, {os.path.basename(path2)}), Actual Label: {label_val}, Predicted Similarity: {sim:.4f} (Same class)\")\n",
        "    else:\n",
        "        print(\"\\nValidation pairs not available for prediction with Phase 2 model.\")\n",
        "else:\n",
        "    print(\"\\nPhase 2 model was not loaded successfully. Skipping predictions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IH50CqBmTRK",
        "outputId": "f0dfe2a8-82e9-47da-8257-278d6a2c26cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 2 fine-tuned Siamese model saved to /content/drive/MyDrive/ColabNotebooks/results/siamese_mobilenetv2_landmark_model_phase2_toplayers.keras (by ModelCheckpoint).\n",
            "This model was built with the corrected Lambda layer definition (with output_shape).\n",
            "Loading the best model from Phase 2 fine-tuning...\n",
            "Unsafe deserialization enabled (to load Python lambda in Lambda layer).\n",
            "Best Phase 2 model loaded successfully.\n",
            "\n",
            "Example predictions on validation pairs (using the Phase 2 fine-tuned model):\n"
          ]
        }
      ]
    }
  ]
}